{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d358f0b-5a84-469a-a7a1-67e36f2e9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ae026-0cc8-4401-b07f-18d7604122b9",
   "metadata": {},
   "source": [
    "## SET THESE VALUES!\n",
    "- **DB_DIR**\n",
    "    - The location of where your databases live\n",
    "        - Should be either OUTPUT_DEFAULT_DIR specified at monitoring program build time, or the --ouptut-dir specfied at runtime\n",
    "- **PREPROCESS_SAVE_DIR**\n",
    "    - Where you want to save the compact, preprocessed numpy representations of the data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bed05b-8354-4b45-8749-cda7190c9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET THESE \n",
    "DB_DIR = \"\"\n",
    "PREPROCESS_SAVE_DIR = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f232ed2-46be-4ea7-ac49-250e97ee2954",
   "metadata": {},
   "source": [
    "## Query Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445ff63-c218-41f0-b512-03f109e7cc6b",
   "metadata": {},
   "source": [
    "### GPU Metrics\n",
    "- Ignoring the PCIe/NVLink For now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04931e2b-ce3d-432c-8d14-f4d0d8bdaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_MEM_UTIL_ID = 254\n",
    "SMI_UTIL_ID = 203\n",
    "SM_ACTIVE_ID = 1002\n",
    "SM_OCCUPANCY_ID = 1003\n",
    "TENSOR_ACTIVE_ID = 1004\n",
    "DRAM_ACTIVE_ID = 1005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74bdd50-fc08-48bc-988c-3a12802067f6",
   "metadata": {},
   "source": [
    "### Fetching data to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7a626-4cc8-44e5-a018-612eb36ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assumes that cur is already a cursor that is connected to a db\n",
    "\n",
    "## where fields is a dict from fields: (name, dtype)\n",
    "def preprocess_gpu_data(cur, fields):\n",
    "\n",
    "    ## note: gpu data will have non-negative device_ids\n",
    "    base_dt = [(\"timestamp\", \"datetime64[ns]\"), (\"device_id\", np.uint8)]\n",
    "    field_arrs = []\n",
    "\n",
    "    print(\"Fetching All Fields...\")\n",
    "    ## cleaner to query each field individually\n",
    "    ##   - instead could get better perf/potential memory savings (depending on number of fields) to \n",
    "    ##     just fetch the whole db, convert to numpy and then filter\n",
    "    for k, v in fields.items():\n",
    "        print(f\"\\t{v[0]}...\")\n",
    "        q = f\"SELECT timestamp, device_id, value FROM Data WHERE field_id = {k}\"\"\"\n",
    "        db_result = cur.execute(q).fetchall()\n",
    "        q_dt = base_dt + [(\"value\", v[1])]\n",
    "        numpy_arr = np.array(db_result, dtype=q_dt)\n",
    "        ## converted to numpy array so can free the list of tuples\n",
    "        del db_result\n",
    "        field_arrs.append((k, numpy_arr))\n",
    "\n",
    "    ## all of these field arrs should have same timestamps and device_ids\n",
    "    base_field_id = field_arrs[0][0]\n",
    "    base_field_name = fields[base_field_id][0]\n",
    "    num_rows = len(field_arrs[0][1])\n",
    "    timestamps = field_arrs[0][1][\"timestamp\"]\n",
    "    device_ids = field_arrs[0][1][\"device_id\"]\n",
    "\n",
    "    ## Error check\n",
    "    for field_num in range(1, len(field_arrs)):\n",
    "        field_id = field_arrs[field_num][0]\n",
    "        field_name = fields[field_id][0]\n",
    "        field_arr = field_arrs[field_num][1]\n",
    "        if len(field_arr) != num_rows:\n",
    "            print(f\"Error processing field {field_name}. Had different length [{len(field_arr)}] than {base_field_name} [{num_rows}].\")\n",
    "            return None\n",
    "        if not np.array_equal(field_arr[\"timestamp\"], timestamps):\n",
    "            print(f\"Error processing field {field_name}. Had different timestamps vs. {base_field_name}.\")\n",
    "            return None\n",
    "        if not np.array_equal(field_arr[\"device_id\"], device_ids):\n",
    "            print(f\"Error processing field {field_name}. Had different device_ids vs. {base_field_name}.\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    ## The field arrays are referencing the same window of time and same devices, so we can proceed\n",
    "    \n",
    "    ## Create output array\n",
    "    print(\"Populating output numpy array...\")\n",
    "    \n",
    "    base_result_dt = [(\"timestamp\", \"datetime64[ns]\"), (\"device_id\", np.uint8)]\n",
    "    result_dt = base_result_dt\n",
    "    for i in range(len(field_arrs)):\n",
    "        field_id = field_arrs[i][0]\n",
    "        result_dt.append(fields[field_id])\n",
    "\n",
    "    ## add diff utils if both 203 and 1002 are within the list\n",
    "    if ((SMI_UTIL_ID in fields) and (SM_ACTIVE_ID in fields)):\n",
    "        result_dt.append((\"diff_util_metrics\", np.uint8))\n",
    "        \n",
    "    result = np.zeros(num_rows, dtype=result_dt)\n",
    "\n",
    "    result[\"timestamp\"] = timestamps\n",
    "    result[\"device_id\"] = device_ids\n",
    "\n",
    "    ## setting the values for field ids\n",
    "    smi_util_ind, sm_active_ind = None, None\n",
    "    for i in range(len(field_arrs)):\n",
    "        field_id = field_arrs[i][0]\n",
    "        field_name = fields[field_id][0]\n",
    "        result[field_name] = field_arrs[i][1][\"value\"]\n",
    "        if field_id == SMI_UTIL_ID:\n",
    "            smi_util_ind = i\n",
    "        if field_id == SM_ACTIVE_ID:\n",
    "            sm_active_ind = i\n",
    "\n",
    "    ## if both compute utilization metrics are being queried also include their difference (for convenience)\n",
    "    if ((SMI_UTIL_ID in fields) and (SM_ACTIVE_ID in fields)):\n",
    "        ## if they are both in fields than their indexes would have been set above\n",
    "        result[\"diff_util_metrics\"] = field_arrs[smi_util_ind][1][\"value\"] - field_arrs[sm_active_ind][1][\"value\"]\n",
    "\n",
    "    ## can delete all the field_arrs now that they have been copied to the result\n",
    "    for i in range(len(field_arrs)):\n",
    "        orig_np_arr = field_arrs[i][1]\n",
    "        del orig_np_arr\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c2d88-8644-4d77-937d-0df69841e039",
   "metadata": {},
   "source": [
    "### Specifying what fields to retrieve to a compact numpy array for fast processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570541ef-a020-4642-9c84-06bc50a107f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {GPU_MEM_UTIL_ID: (\"gpu_mem_used\", np.uint8), SMI_UTIL_ID: (\"smi_util\", np.uint8), SM_ACTIVE_ID: (\"sm_active\", np.uint8), SM_OCCUPANCY_ID: (\"sm_occupancy\", np.uint8),\n",
    "          TENSOR_ACTIVE_ID: (\"tensor_active\", np.uint8), DRAM_ACTIVE_ID: (\"dram_active\", np.uint8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86184db2-2132-4a8c-8246-e82e20d55a50",
   "metadata": {},
   "source": [
    "### Iterating through all nodes and saving results to numpy files that can be easily loaded and analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0564abb-eb54-4227-8587-ee39c6d4e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_db_files = sorted(glob.glob(DB_DIR + \"*.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcaab9-b3cf-4126-b12e-970bac248517",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for node_db_file in node_db_files:\n",
    "\n",
    "    node_name = node_db_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(f\"{i}: {node_name}\")\n",
    "    \n",
    "    ### Reading the DB into Memory \n",
    "    source_con = sqlite3.connect(node_db_file)\n",
    "    con = sqlite3.connect(':memory:')\n",
    "    source_con.backup(con)\n",
    "    ## we copied the db to be in-memory, so we are done with the file\n",
    "    source_con.close()\n",
    "    cur = con.cursor()\n",
    "\n",
    "    ## Getting the compact numpy representation\n",
    "    preprocessed_result = preprocess_gpu_data(cur, fields)\n",
    "\n",
    "    ## save results for future processing\n",
    "    save_np_filename = PREPROCESS_SAVE_DIR + node_name + \".npy\"\n",
    "    with open(save_np_filename, 'wb') as f:\n",
    "        np.save(f, preprocessed_result)\n",
    "\n",
    "    ## can free the numpy memory before next iteration now\n",
    "    del preprocessed_result\n",
    "\n",
    "    ## disconnect from in-memory db\n",
    "    con.close()\n",
    "    \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
